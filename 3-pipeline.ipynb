{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ee19644a-1ce5-47e4-89e7-46ff622ce1bd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import operator as op\n",
    "import re\n",
    "\n",
    "#Pipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# Custom Transformers\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import make_column_selector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1ddeabda-a6d6-4bff-aabe-d9f3bd4f1a5d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"Data/listings.csv\", sep = \",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "470ac0cd-2633-49df-a6a3-5bd6e9076484",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ColumnDroppersTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "        \n",
    "    def transform(self, X, y=None):\n",
    "        \n",
    "        print(\"Start - ColumnDroppersTransformer\")\n",
    "        \n",
    "        # Find features related to host, id and url\n",
    "        re_drop = \".*host.*|.*id.*|.*url.*\"\n",
    "        drop_feat = []\n",
    "        for feat in X.columns:\n",
    "            if re.match(re_drop, feat):\n",
    "                 drop_feat.append(feat)\n",
    "        # Add extra features that for sure will not ber part of the model\n",
    "        drop_feat.extend([\"calendar_updated\", \"license\", \"neighbourhood_group_cleansed\", \"neighbourhood\", \"neighborhood_overview\", \n",
    "                          \"last_scraped\", \"source\", \"first_review\", \"last_review\", \"name\", \"number_of_reviews_l30d\", \"number_of_reviews\",\n",
    "                          \"availability_30\",\"availability_60\",\"availability_90\", \"minimum_nights\", \"maximum_nights\", \"review_scores_value\",  \n",
    "                         \"review_scores_accuracy\", \"review_scores_rating\", \"review_scores_checkin\", \"review_scores_cleanliness\", \"review_scores_communication\",\n",
    "                         \"has_availability\", \"instant_bookable\",])\n",
    "        \n",
    "        X = X.drop(drop_feat, axis = 1)\n",
    "        \n",
    "        print(\"End - ColumnDroppersTransformer\")\n",
    "        \n",
    "        return X\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "class DropNasTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, features):\n",
    "        self.features = features\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        \n",
    "        print(\"Start - DropNasTransformer - transform\")\n",
    "        \n",
    "        \n",
    "        X = X.dropna(subset = self.features)\n",
    "        \n",
    "        print(type(X))\n",
    "        print(\"End - DropNasTransformer - transform\")\n",
    "        \n",
    "        \n",
    "        return X\n",
    "    \n",
    "    def fit(self, X):\n",
    "        \n",
    "        \n",
    "        return self\n",
    "    \n",
    "class OutlierRemover(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, features_limit, mode = \"remove\", operator = \"eq\"): #st: smaller then\n",
    "                    \n",
    "        self.opt_dict = {\n",
    "        \"lt\":op.lt,\n",
    "        \"le\":op.le,\n",
    "        \"eq\":op.eq,\n",
    "        \"ne\":op.ne,\n",
    "        \"ge\":op.ge,\n",
    "        \"gt\":op.gt,\n",
    "        }    \n",
    "        \n",
    "        self.features_limit = features_limit # list of tuples\n",
    "        self.mode = mode  #mode of operation (cap value or remove value)\n",
    "        self.operator = operator #lt, eq, gt, ...\n",
    "     \n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        \n",
    "        print(\"Start - OutlierRemover\")\n",
    "\n",
    "        NAME = 0\n",
    "        LIMIT = 1\n",
    "        if(self.mode == \"remove\"): ###!!! check if it works\n",
    "            for a_feature in self.features_limit:\n",
    "                X = X.drop(X[self.opt_dict[self.operator](X[a_feature[NAME]], a_feature[LIMIT])].index)\n",
    "        elif(self.mode == \"cap\"):\n",
    "            for a_feature in self.features_limit:\n",
    "                X[a_feature[NAME]] =  X[a_feature[NAME]].apply(lambda x : x if self.opt_dict[self.operator](x,a_feature[LIMIT]) else a_feature[LIMIT])\n",
    "        \n",
    "        print(\"End - OutlierRemover\")\n",
    "        \n",
    "        \n",
    "        return X\n",
    "    \n",
    "    def fit(self, X):\n",
    "        return self\n",
    "        \n",
    "        \n",
    "class ClusterGeolocationTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, clusters = 8, init = 'k-means++', n_init = 'auto', max_iter = 300):\n",
    "        \n",
    "        self.clusters = clusters\n",
    "        self.init = init\n",
    "        self.n_init = n_init\n",
    "        self.max_iter = max_iter\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        \n",
    "        print(\"Start - ClusterGeolocationTransformer\")\n",
    "        \n",
    "        # Initialize the model\n",
    "        k_means = KMeans(\n",
    "            init = self.init,\n",
    "            n_clusters = self.clusters,\n",
    "            n_init = self.n_init,\n",
    "            max_iter = self.max_iter,\n",
    "            random_state = 69\n",
    "        )\n",
    "\n",
    "\n",
    "        #Select the data to be clustered\n",
    "        df_kmeans = X.loc[:,[\"latitude\", \"longitude\"]]\n",
    "\n",
    "        # Fitting\n",
    "        k_means.fit(df_kmeans)\n",
    "\n",
    "        #Clusters as a feature\n",
    "        X['cluster'] = k_means.labels_ \n",
    "        \n",
    "        #Drop latitude and longitude as they will not be fed into the model\n",
    "        X = X.drop([\"longitude\", \"latitude\"], axis = 1)\n",
    "        \n",
    "        print(\"End - ClusterGeolocationTransformer\")\n",
    "        \n",
    "        return X\n",
    "    \n",
    "    def fit(self, X):\n",
    "        return self\n",
    "\n",
    "\n",
    "\n",
    "class PreprocessCorpus(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, corpus_feature):\n",
    "        self.corpus_feature = corpus_feature\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        \n",
    "        print(\"Start - PreprocessCorpus\")\n",
    "        \n",
    "        def process_corpus(corpus):\n",
    "            corpus = corpus.lower()\n",
    "            corpus = re.sub(r'[^a-zA-Z ]', ' ', corpus)\n",
    "            corpus = re.sub(r'\\b(the|and|in|with|to|a|of|br|is|from|for|on|this|you|it|has|all|at|de|by|br)\\b', ' ', corpus)\n",
    "            corpus = re.sub(r'br\\b', '', corpus)\n",
    "            return corpus\n",
    "    \n",
    "        X[self.corpus_feature] = X[self.corpus_feature].apply(lambda x : process_corpus(x))\n",
    "        \n",
    "        print(\"End - PreprocessCorpus\")\n",
    "        \n",
    "        return X\n",
    "    \n",
    "    def fit(self, X):\n",
    "        return self    \n",
    "\n",
    "\n",
    "class ContainWordsTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, words, new_feature_name, corpus_target):\n",
    "        self.words = words\n",
    "        self.feature_name = new_feature_name\n",
    "        self.corpus_target = corpus_target\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        \n",
    "        print(\"Start - ContainWordsTransformer\")\n",
    "        \n",
    "        def check_lux_corpus(x):\n",
    "            \n",
    "            # Transform list of words in regex\n",
    "            w_list = []\n",
    "            for i in self.words:\n",
    "                w_list += w_list + i + \"|\"\n",
    "            #remove last \"|\"\n",
    "            w_list.pop()  \n",
    "            regex = r'\\b('+w_list+')\\b'\n",
    "            \n",
    "            # Return if description contains lux word or not\n",
    "            if (re.search(regex, x)):\n",
    "                return 1\n",
    "            else:\n",
    "                return 0\n",
    "        \n",
    "        #Create the feature that identify a property as luxurious\n",
    "        X[self.new_feature_name] = X[self.corpus_target].apply(check_lux_corpus)\n",
    "        \n",
    "        # Drop corpus target feature\n",
    "        X = X.drop(self.corpus_target, axis = 1)\n",
    "        \n",
    "        print(\"End - ContainWordsTransformer\")\n",
    "        \n",
    "        return X\n",
    "    \n",
    "    def fit(self, X):\n",
    "        return self\n",
    "    \n",
    "    \n",
    "class ExtractAmenitiesTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, amenities_dict):\n",
    "        self.amenities_dict = amenities_dict\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        \n",
    "        print(\"Start - ExtractAmenitiesTransformer\")\n",
    "        \n",
    "        #This function will check the presence of the amenities in the list using regex\n",
    "        def convert_amenities(amenities, index): \n",
    "            for amn_name, amn_re in self.amenities_dict.items():\n",
    "                if(isinstance(amn_re,list)):\n",
    "                    # Check each reg from the list of reg for an specific amenity. Ex (\"seaview\" : [\".*beach view.*\",\".*sea view.*\",\".*ocean view.*\"])\n",
    "                    for reg in amn_re:\n",
    "                        if(re.match(reg, str.lower(amenities))):\n",
    "                            X.loc[\"index,has_\"+ amn_name] = 1\n",
    "                else:\n",
    "                    if(re.match(amn_re, str.lower(amenities))):\n",
    "                        X.loc[index,\"has_\"+ amn_name] = 1\n",
    "        \n",
    "        # 1) Create and initalize the features from the amenities list\n",
    "        has_amn_feat = [\"has_\" + x for x in self.amenities_dict.keys()]\n",
    "        X.has_amn_feat = 0\n",
    "        \n",
    "        # 2) For each amenities' list, check if they contain the amenities target\n",
    "        for index, row in X.iterrows():\n",
    "            convert_amenities(X.loc[index, \"amenities\"], index)\n",
    "        \n",
    "        # 3) Drop 'amenities' feature\n",
    "        X = X.drop(\"amenities\", axis = 1)\n",
    "        \n",
    "        print(\"End - ExtractAmenitiesTransformer\")\n",
    "        \n",
    "        return X\n",
    "    \n",
    "    def fit(self, X):\n",
    "        return self\n",
    "    \n",
    "    \n",
    "   \n",
    "    \n",
    "class ExtractBathroom(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "\n",
    "        print(\"Start - ExtractBathroom\")\n",
    "        \n",
    "        def bathroom_number(a_bath):\n",
    "            \n",
    "            is_shared = 0 #default value\n",
    "\n",
    "            # Case if it is a missing value\n",
    "            if(pd.isna(a_bath)):\n",
    "                number_bath = np.nan\n",
    "                is_shared = np.nan\n",
    "                \n",
    "            # Check if bathroom is shared using regex\n",
    "            else:\n",
    "                if(re.match(\".*shared.*\", str.lower(a_bath))):\n",
    "                        is_shared = 1  \n",
    "\n",
    "\n",
    "            return is_shared\n",
    "            \n",
    "    \n",
    "\n",
    "        X[[\"is_bathroom_shared\"]] = X[\"bathrooms_text\"].apply(lambda x: pd.Series(bathroom_number(x)))\n",
    "        \n",
    "        X = X.drop(\"bathroom_text\", axis = 1)\n",
    "        \n",
    "        print(\"End - ExtractBathroom\")\n",
    "        \n",
    "        return X\n",
    "    \n",
    "    def fit(self, X):\n",
    "        return self\n",
    "    \n",
    "\n",
    "class CatImputer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, features_limits):\n",
    "        self.features_limits = features_limits\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        \n",
    "        print(\"Start - CatImputer\")\n",
    "        \n",
    "        FILL_VALUE = 1\n",
    "        FEATURE = 0\n",
    "        X_imp = X\n",
    "        for feat in self.features_limits:\n",
    "            imp = SimpleImputer(strategy='constant', fill_value= feat[FILL_VALUE])\n",
    "            X_imp = imp.fit_transform(X_imp)\n",
    "        \n",
    "        print(\"End - CatImputer\")\n",
    "        \n",
    "        return pd.DataFrame(X_imp, columns = X.columns)\n",
    "    \n",
    "    def fit(self, X):\n",
    "        return self  \n",
    "    \n",
    "    \n",
    "class PriceTypeTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        \n",
    "        print(\"Start - PriceTypeTransformer - transform\")\n",
    "        \n",
    "        X[\"price\"] = X[\"price\"].apply(lambda x: float(x[1:].replace(\",\",\"\")) if pd.notna(x) else x)\n",
    "        \n",
    "        print(\"End - PriceTypeTransformer - transform\")\n",
    "        \n",
    "        return X\n",
    "    \n",
    "    def fit(self, X):\n",
    "        return self  \n",
    "    \n",
    "\n",
    "class NumImputer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, value):\n",
    "        self.value = value\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        \n",
    "        print(\"Start - FeaturesImputer - transform\")\n",
    "        \n",
    "        impt = SimpleImputer(strategy='constant', fill_value = self.value)\n",
    "        impt_X = impt.fit_transform(X)\n",
    "        \n",
    "        print(\"End - FeaturesImputer - transform\")\n",
    "        \n",
    "        \n",
    "        return pd.DataFrame(impt_X, columns = X.columns)\n",
    "    \n",
    "    def fit(self, X):\n",
    "        \n",
    "        \n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e8f68edc-c923-4ab5-8ee8-167413e37a94",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "target_feature = \"price\"\n",
    "\n",
    "\n",
    "# Separte target feature from the dataset\n",
    "#X = df.drop(target_feature, axis = 1)\n",
    "#y = df[target_feature]\n",
    "\n",
    "X = df\n",
    "\n",
    "\n",
    "num_feat_extraction_pipeline = Pipeline([\n",
    "    (\"clusterlocation\", ClusterGeolocationTransformer(clusters = 25, init = \"random\", n_init = 15, max_iter = 1000)),\n",
    "    \n",
    "       \n",
    "    # binary is_score_empty (?) - maybe info is reduntant\n",
    "])\n",
    "\n",
    "##################################\n",
    "##################################\n",
    "\n",
    "cat_feat_extraction_pipeline = Pipeline([\n",
    "    #Removes capital letters, ponctuation, etc...\n",
    "    (\"preprocesstext\", PreprocessCorpus(\"description\")),\n",
    "     #Creates a new feature based on lux words\n",
    "    (\"extractluxdescription\", ContainWordsTransformer(new_feature_name = \"contains_lux_description\", \n",
    "                                               corpus_target = \"description\",\n",
    "                                               words = [\"lux\",\"luxurious\",\"luxury\",\"fancy\",\"garage\", \n",
    "                                                  \"hydromassage\", \"cellar\", \"sophistication\", \n",
    "                                                  \"magnificent\", \"colonial\", \"rooftop\", \"triplex\", \"suite\"])), \n",
    "    \n",
    "    (\"extractamenities\", ExtractAmenitiesTransformer(\n",
    "                                                { \"parking\": \".*parking on premises.*\",\n",
    "                                                  \"pool\":\".*pool.*(?!.*\\btable\\b).*\",\n",
    "                                                  \"washer\": \".*washer.*\",\n",
    "                                                  \"dishwasher\": \".*dishwasher.*\",\n",
    "                                                 \"ceiling_fan\" : \".*ceiling fan.*\",\n",
    "                                                 \"long_term\" : \".*long term.*\",\n",
    "                                                 \"bbq_grill\" : \".*bbq grill.*\",\n",
    "                                                 \"outdoor\": \".*outdoor.*\",\n",
    "                                                 \"hot_tub\": \".*hot tub.*\",\n",
    "                                                 \"bathtub\": \".*bathtub.*\",\n",
    "                                                 \"ac\": [\".*air conditioning.*\",\"\\\\bac\\\\b\"],\n",
    "                                                 \"seaview\" : [\".*beach view.*\",\".*sea view.*\",\".*ocean view.*\"]\n",
    "                                                }\n",
    "    )),\n",
    "    \n",
    "    (\"extractbathroom\", ExtractBathroom()),\n",
    "    (\"extractroomtype\", OneHotEncoder(sparse_output = False, feature_name_combiner='concat')),\n",
    "])\n",
    "\n",
    "##################################\n",
    "##################################\n",
    "\n",
    "num_pipeline = Pipeline([\n",
    "    (\"dropnas\", DropNasTransformer(\"price\")),\n",
    "    (\"num_imputer\", NumImputer(value = 0)),\n",
    "    (\"outliers\", OutlierRemover(\n",
    "                                features_limit = [(\"price\", 1000),\n",
    "                                                 (\"minimum_nights_avg_ntm\", 7),\n",
    "                                                 (\"beds\", 8),\n",
    "                                                 (\"bedrooms\", 5),\n",
    "                                                 (\"bathrooms\", 5),\n",
    "                                                 (\"accommodates\", 10),\n",
    "                                                 (\"number_of_reviews_ltm\", 25),\n",
    "                                                 (\"reviews_per_month\", 4),\n",
    "                                                ],\n",
    "                                mode = \"cap\",\n",
    "                                operator = \"lt\" #operation: less then (lt)\n",
    "                                \n",
    "                                )),\n",
    "    \n",
    "])\n",
    "\n",
    "##################################\n",
    "##################################\n",
    "\n",
    "cat_pipeline = Pipeline([\n",
    "    (\"cat_imputer\", CatImputer(\n",
    "                            features_limits = [(\"bathroom_text\", \"Private bath\"),\n",
    "                                               (\"description\", \"\"),\n",
    "                                              ]\n",
    "    )),\n",
    "    (\"outliers\", OutlierRemover(\n",
    "                                features_limit = [(\"room_type\", \"Hotel room\")],\n",
    "                                mode = \"remove\",\n",
    "                                operator = \"eq\",\n",
    "    )),\n",
    "\n",
    "])\n",
    "\n",
    "##################################\n",
    "##################################\n",
    "\n",
    "\n",
    "preprocessor_pipeline = Pipeline([   \n",
    "    ('pricetype', PriceTypeTransformer()),\n",
    "    ('preprocessor', ColumnTransformer([     \n",
    "                                        (\"num_ct\", num_pipeline, make_column_selector(dtype_include = np.number)), #make_column_selector (to dinamically get the columns names)\n",
    "                                        (\"cat_ct\", cat_pipeline, make_column_selector(dtype_exclude = np.number)),\n",
    "                                    ])), \n",
    "    ('initialdrop', ColumnDroppersTransformer()), #Add this to feature_selection (?) (!)\n",
    "])\n",
    "\n",
    "##################################\n",
    "##################################\n",
    "\n",
    "#onehotencoder = ColumnTransformer([\"onehotencoder\",\n",
    "#                                          OneHotEncoder(sparse_output = False, feature_name_combiner='concat'),\n",
    "#                                          X.select_dtypes(include = \"object\")])\n",
    "\n",
    "model_pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor_pipeline),\n",
    "    (\"num_feature_extraction\", num_feat_extraction_pipeline),\n",
    "    (\"cat_feat_extraction_pipeline\", cat_feat_extraction_pipeline),        \n",
    "    (\"onehotencoder\", ColumnTransformer([\n",
    "                                            (OneHotEncoder(sparse_output = False, feature_name_combiner='concat'), X.select_dtypes(include = \"object\"))\n",
    "                                        ])),\n",
    "\n",
    "    (\"normalisation\", MinMaxScaler()),\n",
    "    # Feature selection (remove low correaltion, low impaact, etc...)\n",
    "    # Model training\n",
    "    \n",
    "])\n",
    "\n",
    "##################################\n",
    "##################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3efe9ed7-db4a-4625-ae42-6700e0b1aeb4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start - PriceTypeTransformer - transform\n",
      "End - PriceTypeTransformer - transform\n",
      "Start - DropNasTransformer - transform\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "End - DropNasTransformer - transform\n",
      "Start - FeaturesImputer - transform\n",
      "End - FeaturesImputer - transform\n",
      "Start - OutlierRemover\n",
      "End - OutlierRemover\n",
      "Start - CatImputer\n",
      "End - CatImputer\n",
      "Start - OutlierRemover\n",
      "End - OutlierRemover\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "all the input array dimensions except for the concatenation axis must match exactly, but along dimension 0, the array at index 0 has size 33692 and the array at index 1 has size 34639",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#Training\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m model_pipeline\u001b[38;5;241m.\u001b[39mfit(X)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1471\u001b[0m     )\n\u001b[0;32m   1472\u001b[0m ):\n\u001b[1;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\pipeline.py:469\u001b[0m, in \u001b[0;36mPipeline.fit\u001b[1;34m(self, X, y, **params)\u001b[0m\n\u001b[0;32m    426\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Fit the model.\u001b[39;00m\n\u001b[0;32m    427\u001b[0m \n\u001b[0;32m    428\u001b[0m \u001b[38;5;124;03mFit all the transformers one after the other and sequentially transform the\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    466\u001b[0m \u001b[38;5;124;03m    Pipeline with fitted steps.\u001b[39;00m\n\u001b[0;32m    467\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    468\u001b[0m routed_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_method_params(method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit\u001b[39m\u001b[38;5;124m\"\u001b[39m, props\u001b[38;5;241m=\u001b[39mparams)\n\u001b[1;32m--> 469\u001b[0m Xt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit(X, y, routed_params)\n\u001b[0;32m    470\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _print_elapsed_time(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPipeline\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_log_message(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m)):\n\u001b[0;32m    471\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_final_estimator \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpassthrough\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\pipeline.py:406\u001b[0m, in \u001b[0;36mPipeline._fit\u001b[1;34m(self, X, y, routed_params)\u001b[0m\n\u001b[0;32m    404\u001b[0m     cloned_transformer \u001b[38;5;241m=\u001b[39m clone(transformer)\n\u001b[0;32m    405\u001b[0m \u001b[38;5;66;03m# Fit or load from cache the current transformer\u001b[39;00m\n\u001b[1;32m--> 406\u001b[0m X, fitted_transformer \u001b[38;5;241m=\u001b[39m fit_transform_one_cached(\n\u001b[0;32m    407\u001b[0m     cloned_transformer,\n\u001b[0;32m    408\u001b[0m     X,\n\u001b[0;32m    409\u001b[0m     y,\n\u001b[0;32m    410\u001b[0m     \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    411\u001b[0m     message_clsname\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPipeline\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    412\u001b[0m     message\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_log_message(step_idx),\n\u001b[0;32m    413\u001b[0m     params\u001b[38;5;241m=\u001b[39mrouted_params[name],\n\u001b[0;32m    414\u001b[0m )\n\u001b[0;32m    415\u001b[0m \u001b[38;5;66;03m# Replace the transformer of the step with the fitted\u001b[39;00m\n\u001b[0;32m    416\u001b[0m \u001b[38;5;66;03m# transformer. This is necessary when loading the transformer\u001b[39;00m\n\u001b[0;32m    417\u001b[0m \u001b[38;5;66;03m# from the cache.\u001b[39;00m\n\u001b[0;32m    418\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps[step_idx] \u001b[38;5;241m=\u001b[39m (name, fitted_transformer)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\joblib\\memory.py:349\u001b[0m, in \u001b[0;36mNotMemorizedFunc.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    348\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 349\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\pipeline.py:1310\u001b[0m, in \u001b[0;36m_fit_transform_one\u001b[1;34m(transformer, X, y, weight, message_clsname, message, params)\u001b[0m\n\u001b[0;32m   1308\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _print_elapsed_time(message_clsname, message):\n\u001b[0;32m   1309\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(transformer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit_transform\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m-> 1310\u001b[0m         res \u001b[38;5;241m=\u001b[39m transformer\u001b[38;5;241m.\u001b[39mfit_transform(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit_transform\u001b[39m\u001b[38;5;124m\"\u001b[39m, {}))\n\u001b[0;32m   1311\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1312\u001b[0m         res \u001b[38;5;241m=\u001b[39m transformer\u001b[38;5;241m.\u001b[39mfit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit\u001b[39m\u001b[38;5;124m\"\u001b[39m, {}))\u001b[38;5;241m.\u001b[39mtransform(\n\u001b[0;32m   1313\u001b[0m             X, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransform\u001b[39m\u001b[38;5;124m\"\u001b[39m, {})\n\u001b[0;32m   1314\u001b[0m         )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1471\u001b[0m     )\n\u001b[0;32m   1472\u001b[0m ):\n\u001b[1;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\pipeline.py:533\u001b[0m, in \u001b[0;36mPipeline.fit_transform\u001b[1;34m(self, X, y, **params)\u001b[0m\n\u001b[0;32m    490\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Fit the model and transform with the final estimator.\u001b[39;00m\n\u001b[0;32m    491\u001b[0m \n\u001b[0;32m    492\u001b[0m \u001b[38;5;124;03mFit all the transformers one after the other and sequentially transform\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    530\u001b[0m \u001b[38;5;124;03m    Transformed samples.\u001b[39;00m\n\u001b[0;32m    531\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    532\u001b[0m routed_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_method_params(method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit_transform\u001b[39m\u001b[38;5;124m\"\u001b[39m, props\u001b[38;5;241m=\u001b[39mparams)\n\u001b[1;32m--> 533\u001b[0m Xt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit(X, y, routed_params)\n\u001b[0;32m    535\u001b[0m last_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_final_estimator\n\u001b[0;32m    536\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _print_elapsed_time(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPipeline\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_log_message(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m)):\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\pipeline.py:406\u001b[0m, in \u001b[0;36mPipeline._fit\u001b[1;34m(self, X, y, routed_params)\u001b[0m\n\u001b[0;32m    404\u001b[0m     cloned_transformer \u001b[38;5;241m=\u001b[39m clone(transformer)\n\u001b[0;32m    405\u001b[0m \u001b[38;5;66;03m# Fit or load from cache the current transformer\u001b[39;00m\n\u001b[1;32m--> 406\u001b[0m X, fitted_transformer \u001b[38;5;241m=\u001b[39m fit_transform_one_cached(\n\u001b[0;32m    407\u001b[0m     cloned_transformer,\n\u001b[0;32m    408\u001b[0m     X,\n\u001b[0;32m    409\u001b[0m     y,\n\u001b[0;32m    410\u001b[0m     \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    411\u001b[0m     message_clsname\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPipeline\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    412\u001b[0m     message\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_log_message(step_idx),\n\u001b[0;32m    413\u001b[0m     params\u001b[38;5;241m=\u001b[39mrouted_params[name],\n\u001b[0;32m    414\u001b[0m )\n\u001b[0;32m    415\u001b[0m \u001b[38;5;66;03m# Replace the transformer of the step with the fitted\u001b[39;00m\n\u001b[0;32m    416\u001b[0m \u001b[38;5;66;03m# transformer. This is necessary when loading the transformer\u001b[39;00m\n\u001b[0;32m    417\u001b[0m \u001b[38;5;66;03m# from the cache.\u001b[39;00m\n\u001b[0;32m    418\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps[step_idx] \u001b[38;5;241m=\u001b[39m (name, fitted_transformer)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\joblib\\memory.py:349\u001b[0m, in \u001b[0;36mNotMemorizedFunc.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    348\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 349\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\pipeline.py:1310\u001b[0m, in \u001b[0;36m_fit_transform_one\u001b[1;34m(transformer, X, y, weight, message_clsname, message, params)\u001b[0m\n\u001b[0;32m   1308\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _print_elapsed_time(message_clsname, message):\n\u001b[0;32m   1309\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(transformer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit_transform\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m-> 1310\u001b[0m         res \u001b[38;5;241m=\u001b[39m transformer\u001b[38;5;241m.\u001b[39mfit_transform(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit_transform\u001b[39m\u001b[38;5;124m\"\u001b[39m, {}))\n\u001b[0;32m   1311\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1312\u001b[0m         res \u001b[38;5;241m=\u001b[39m transformer\u001b[38;5;241m.\u001b[39mfit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit\u001b[39m\u001b[38;5;124m\"\u001b[39m, {}))\u001b[38;5;241m.\u001b[39mtransform(\n\u001b[0;32m   1313\u001b[0m             X, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransform\u001b[39m\u001b[38;5;124m\"\u001b[39m, {})\n\u001b[0;32m   1314\u001b[0m         )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_set_output.py:316\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    314\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[0;32m    315\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 316\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m f(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    317\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    318\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    319\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    320\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m    321\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[0;32m    322\u001b[0m         )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1471\u001b[0m     )\n\u001b[0;32m   1472\u001b[0m ):\n\u001b[1;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\compose\\_column_transformer.py:1006\u001b[0m, in \u001b[0;36mColumnTransformer.fit_transform\u001b[1;34m(self, X, y, **params)\u001b[0m\n\u001b[0;32m   1003\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_output(Xs)\n\u001b[0;32m   1004\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_record_output_indices(Xs)\n\u001b[1;32m-> 1006\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_hstack(\u001b[38;5;28mlist\u001b[39m(Xs), n_samples\u001b[38;5;241m=\u001b[39mn_samples)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\compose\\_column_transformer.py:1200\u001b[0m, in \u001b[0;36mColumnTransformer._hstack\u001b[1;34m(self, Xs, n_samples)\u001b[0m\n\u001b[0;32m   1190\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1191\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConcatenating DataFrames from the transformer\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms output lead to\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1192\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m an inconsistent number of samples. The output may have Pandas\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1195\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m samples.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1196\u001b[0m         )\n\u001b[0;32m   1198\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output\n\u001b[1;32m-> 1200\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mhstack(Xs)\n",
      "File \u001b[1;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36mhstack\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\numpy\\core\\shape_base.py:370\u001b[0m, in \u001b[0;36mhstack\u001b[1;34m(tup, dtype, casting)\u001b[0m\n\u001b[0;32m    368\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _nx\u001b[38;5;241m.\u001b[39mconcatenate(arrs, \u001b[38;5;241m0\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mdtype, casting\u001b[38;5;241m=\u001b[39mcasting)\n\u001b[0;32m    369\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 370\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _nx\u001b[38;5;241m.\u001b[39mconcatenate(arrs, \u001b[38;5;241m1\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mdtype, casting\u001b[38;5;241m=\u001b[39mcasting)\n",
      "File \u001b[1;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36mconcatenate\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: all the input array dimensions except for the concatenation axis must match exactly, but along dimension 0, the array at index 0 has size 33692 and the array at index 1 has size 34639"
     ]
    }
   ],
   "source": [
    "#Training\n",
    "model_pipeline.fit(X)\n",
    "\n",
    "#Predict\n",
    "#pred = model_pipeline.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea6ac581-b957-4bc8-9f69-c0a7192a1b40",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c401904-f052-4dc1-8e3d-b1f1f6a7836d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d056dda-2fcf-472f-bcaf-b49375b0036f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc91b37a-8e83-47ab-8317-2ae921990a0a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4226147-3b84-4df1-a5e3-6616f7af6321",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Custom transformers (for template purposes of how pipeline works)\n",
    "\n",
    "#Drop columns\n",
    "class ColumnDropperTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, columns):\n",
    "        self.columns = columns\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        X = X.drop(self.columns, axis = 1)\n",
    "        return X\n",
    "    \n",
    "    def fit(self, X):\n",
    "        return self\n",
    "    \n",
    "#Filter\n",
    "class DropNas(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        X = X.dropna(subset = \"bathrooms\")\n",
    "        X = X.dropna(subset = [\"bedrooms\", \"beds\"])\n",
    "        X = X.dropna(subset = \"price\")\n",
    "        \n",
    "        return X\n",
    "    \n",
    "    def fit(self, X):\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f9afcc-ec8d-4e42-92a0-db78a49fa474",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "drop_columns = [\"host_id\", \"host_url\", \"host_name\", \"host_location\", \"host_about\", \"host_thumbnail_url\", \n",
    "        \"host_picture_url\", \"host_neighbourhood\", \"host_since\", \"host_listings_count\", \"host_total_listings_count\",\n",
    "        \"id\", \"scrape_id\", \"listing_url\", \"picture_url\", \"minimum_minimum_nights\", \"maximum_minimum_nights\", \"minimum_maximum_nights\", \n",
    "        \"maximum_maximum_nights\", \"minimum_nights_avg_ntm\", \"maximum_nights_avg_ntm\", \"last_scraped\", \"source\", \"first_review\", \"last_review\", \"license\", \"neighbourhood\",\n",
    "        \"neighborhood_overview\", \"neighbourhood_group_cleansed\", \"name\", \"description\",\n",
    "        \"calendar_updated\", \"calculated_host_listings_count\", \"calculated_host_listings_count_entire_homes\",\n",
    "        \"calculated_host_listings_count_private_rooms\", \"calculated_host_listings_count_shared_rooms\", \"calendar_last_scraped\", \"longitude\", \"latitude\",\"availability_365\", \"minimum_nights\", \n",
    "        \"maximum_nights\", \"availability_30\",\"availability_60\", \"availability_90\", \"number_of_reviews_ltm\", \"number_of_reviews_l30d\", \"number_of_reviews\",\n",
    "               \"review_scores_value\", \"review_scores_rating\", \"review_scores_accuracy\",\n",
    "               \"review_scores_checkin\", \"review_scores_cleanliness\", \"review_scores_communication\",\n",
    "               \"review_scores_location\" ,\"reviews_per_month\", \"has_availability\", \"instant_bookable\", \"host_response_rate\", \"host_acceptance_rate\", \"host_is_superhost\", \"host_has_profile_pic\", \"host_identity_verified\",\n",
    "                \"host_response_time\", \"property_type\", \"host_verifications\"\n",
    "                \n",
    "                \n",
    "       ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c28053-d21e-4d16-a151-1f52f2eb7995",
   "metadata": {
    "tags": []
   },
   "source": [
    "drop_transformer = ColumnDropperTransformer(drop_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1213379b-ad18-4bc6-b36a-6bb5911896ad",
   "metadata": {
    "tags": []
   },
   "source": [
    "df_transformed = drop_transformer.fit_transform(df)\n",
    "print(df_transformed.isna().sum())\n",
    "df_transformed.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af9d188-04c8-4a4d-b919-05a944853be5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Option 1: Naming the transformers\n",
    "my_pipeline_named = Pipeline([\n",
    "    (\"dropnas\", DropNas()),\n",
    "    (\"dropcolumns\", ColumnDropperTransformer(columns = drop_columns)),\n",
    "])\n",
    "\n",
    "#Option 2: NOT naming the transformers\n",
    "my_pipeline_unamed = make_pipeline(DropNas(), \n",
    "                            ColumnDropperTransformer(columns = drop_columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5163cbb-bf72-465c-9e44-ef5fa6383273",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_transformed = my_pipeline_named.fit_transform(df)\n",
    "df_transformed.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a73d7ffd-8aa8-4e35-ba8d-cefbc00b70f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "Implement the idea of the code bellow where we separate ccategorical and numerical data and apply5 different transofrmes to each (!)\n",
    "using ColumnTransformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49924cf2-fda2-434a-8eee-908dadcf66f0",
   "metadata": {},
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "num_attribs = [\"longitude\", \"latitude\", \"housing_median_age\", \"total_rooms\",\n",
    "               \"total_bedrooms\", \"population\", \"households\", \"median_income\"]\n",
    "cat_attribs = [\"ocean_proximity\"]\n",
    "\n",
    "cat_pipeline = make_pipeline(\n",
    "    SimpleImputer(strategy=\"most_frequent\"),\n",
    "    OneHotEncoder(handle_unknown=\"ignore\"))\n",
    "\n",
    "preprocessing = ColumnTransformer([\n",
    "    (\"num\", num_pipeline, num_attribs),\n",
    "    (\"cat\", cat_pipeline, cat_attribs),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0842ec5a-99bf-49c2-820b-3389d8a3b2b6",
   "metadata": {},
   "source": [
    "# ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90545dc9-327f-47fe-945d-cbcb41d112ac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "re_drop = \".*host.*|.*id.*|.*url.*\"\n",
    "\n",
    "drop_feat = []\n",
    "\n",
    "for feat in df.columns:\n",
    "    if re.match(re_drop, feat):\n",
    "         drop_feat.append(feat)\n",
    "            \n",
    "drop_feat.extend([\"calendar_updated\", \"license\", \"neighbourhood_group_cleansed\", \"neighbourhood\", \"neighborhood_overview\", \"last_scraped\", \"source\", \"first_review\", \"last_review\", \"name\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2710934-9e76-4c33-b9eb-0d34bdb5562d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d67ca381-8d3d-47c0-bc7e-a11860f439b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
